---
title: "Building a Movie Recommendation System using the MovieLens 10M Dataset"
subtitle: "Harvardx PH125.9x Capstone Project"
author: "Tibor Nagy"
date: "4/12/2021"
output:
  pdf_document: 
    toc: true
    toc_depth: 2
  html_document:
    df_print: paged
  word_document: default
  urlcolor: blue
---

\pagebreak

```{r Required packages, message=FALSE, warning=FALSE, include=FALSE}
# List of packages for session
.packages = c("tidyverse", "knitr", "caret", "data.table", "lubridate")

# Install missing packages
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) install.packages(.packages[!.inst])

# Load packages  
lapply(.packages, require, character.only=TRUE)

# Increase table/plot resolution
knitr::opts_chunk$set(dpi=300)

```

```{r Loading_data, message=FALSE, warning=FALSE, include=FALSE}
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
# movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                     #      title = as.character(title),
                                     #      genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```


# 1.	Introduction

The purpose of this project is to use a publicly available dataset to build a movie recommendation system as a part of the Harvardx Professional Data Scientist program. The project is based on the “MovieLens 10M Dataset” which is released by GroupLens research lab in the Department of Computer Science and Engineering at the University of Minnesota.  
The dataset is a part of the much larger Movielens dataset, it contains 10 million ratings and 100,000 tag applications applied to 10,000 movies by 72,000 users. It is widely used in education, because working with this dataset provides a great opportunity to the students to exercise and develop their data science skills such as data wrangling, visualization, data analysis.  
The dataset is divided to two subsets by the R code provided by Harvardx. The larger subset (called “edx”) contains the 90% of the data and can be used as a training set during the development of the machine learning algorithm. The smaller subset (called “validation”) contains the remaining data. It cannot be used for training purposes, it is only for validation of the final algorithm.  
In this project we try to build a recommendation system, which predict user ratings on the validation data. The goal during the training of the algorithm is to achieve as small RMSE (Root Mean Squared Error) as possible. The final goal is to achieve RMSE < 0. 86490.


# 2. Exploratory Data Analysis

At first, we need to be familiarized with the two datasets (”edx” and “validation”) we are working with.

## 2.1 Dataset Dimensions

We will use the “edx” as the training set and save the “validation” for evaluating the RMSE of the final algorithm.

<div align="center">
</div>
```{r Table1_Dataset_Dimensions, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(fig.pos = "!H")
#Table 1
Table_1 <- data.frame(
  Dataset = c("edx", "validation"),
  No_of_Rows = c(nrow(edx), nrow(validation)),
  No_of_Cols = c(ncol(edx), ncol(validation)))

knitr::kable(
  Table_1,
  format = "pipe",
  caption = "Dataset Dimensions",
  col.names = c("Dataset", "No. of Rows", "No of Columns")
)
```


## 2.2 Missing Data

It is important to know if the dataset contains any missing values, because they can cause us difficulties during the algorithm development, so we need to address them. 


```{r Table2_Missing_Data, echo=FALSE, message=FALSE, warning=FALSE}
sapply(edx, {function(x) sum(is.na(x))}) %>%  
  knitr::kable(format = "pipe", caption = "Number of missing data in each columns")
```

The above table tells us we are lucky, there are no missing values.

## 2.3 Dataset Structure

The datasets are in tidy format. They contain six columns (features) and give us the following information:

* userId \<integer\>: the users are anonymized. The feature contains a unique ID for each user
* movieId \<numeric\>: contains unique ID for each movie
* rating \<numeric\>: contains the rating given by the user to the movie. Ratings are scales from 0.5 to 5 with 0.5 increments
* timestamp \<integer\>: contains the timestamp for the time of rating
* title \<character\>: contains the title of the movie and the year of release
* genres \<character\>: a pipe-separated list, describes the genre or genres that the movie belongs to


```{r Table3_Dataset_Structure, echo=FALSE, message=FALSE, warning=FALSE}
edx %>%
  head() %>% 
  knitr::kable(format = "pipe", caption = "Preview of the dataset")
```


## 2.4 Distribution of Ratings

```{r Plot1_Rating_Distribution, echo=FALSE, message=FALSE, warning=FALSE}
edx %>% group_by(rating) %>%
  summarise(number = n())%>%
  arrange(desc(number)) %>%
  ggplot(aes(rating, number)) +
  geom_bar(stat = "identity", fill = "turquoise4") +
  labs(title="Number of Ratings", x ="Rating", y = "Number")
```
The above chart tells as that the half star ratings are less common than the whole star ratings. The distribution is biased towards the high ratings. 3-star rating is the most common, followed by and 4-star and 5-star ratings.   
The reason of the relatively small number of half star ratings is that the dataset is combined from two smaller datasets. The ratings before 2003 are scales from 1 to 5 star with whole star increments. The half star increments were introduced in 2003. 

```{r Plot2, echo=FALSE, message=FALSE, warning=FALSE, fig.show="hold", out.width="50%"}
edx %>% 
  filter(format(as_datetime(timestamp), "%Y") < 2003) %>%
  group_by(rating) %>%
  summarise(number = n())%>%
  arrange(desc(number)) %>%
  ggplot(aes(rating, number)) +
  geom_bar(stat = "identity", fill = "tomato3") +
  labs(title="Number of Ratings before 2003", x ="Rating", y = "Number")

edx %>% 
  filter(format(as_datetime(timestamp), "%Y") >= 2003) %>%
  group_by(rating) %>%
  summarise(number = n())%>%
  arrange(desc(number)) %>%
  ggplot(aes(rating, number)) +
  geom_bar(stat = "identity", fill = "tomato3") +
  labs(title="Number of Ratings from 2003", x ="Rating", y = "Number")
```

## 2.5 Distribution of Genres
In the "Genres" column contains pipe-separated lists, selected from several genres. All the possible genres and their distribution can be seen in the chart below:

```{r Plot3, echo=FALSE, message=FALSE, warning=FALSE}
separated_genres <- edx %>%
  separate_rows(genres, sep = "\\|")

separated_genres %>%
  group_by(genres) %>%
  summarise(number = n()) %>%
  ggplot(aes(x = reorder(genres, number), y = number)) +
  geom_bar(stat = "identity", fill = "lightslateblue") +
  labs(title="Number of Ratings per Genre", x = "Genre", y = "Number of Ratings") +
  coord_flip()
```

From the plot above we see that the distribution of genres is not even. Drama, Comedy and Action are the most common genres.

## 2.6 Best and worst movies

```{r Table4_10_Best_Movies, echo=FALSE, message=FALSE, warning=FALSE}
edx %>%
  group_by(title) %>%
  summarise(Rating = round(mean(rating), digits = 1), No_of_Ratings = n()) %>%
  arrange(desc(Rating)) %>%
  slice(1:10) %>%
  knitr::kable(format = "pipe", 
               caption = "10 Best movies",
               col.names = c("Title", "Mean Rating", "Number of Ratings"))
```

```{r Table5_10_Worst_Movies, echo=FALSE, message=FALSE, warning=FALSE}
edx %>%
  group_by(title) %>%
  summarise(Rating = round(mean(rating), digits = 1), No_of_Ratings = n()) %>%
  arrange(Rating) %>%
  slice(1:10)%>%
  knitr::kable(format = "pipe", 
               caption = "10 Worst movies", 
               col.names = c("Title", "Mean Rating", "Number of Ratings"))

```
The above tables list the 10 worst and the 10 best movies and their average ratings as well as the number of ratings for each movie. We see that these movies were rated by very few users, they are mostly obscure ones. During the development of our algorithm, we need to address this size effect because it can negatively affect the accuracy of our algorithm. Predictions based on only a few ratings increase the uncertainty.


# 3. Models
In this chapter we start from the simplest possible algorithm (predicting the same rating for all movies regardless of user), and continue with more and more complex models until we obtain the desired RMSE < 0.86490.

We will use the "edx" dataset to train the algorithms, we save the validation set for final evaluation. The edx set will be partitioned to a train_set and a test_set. They will contain the 80% and the remaining 20% of the data respectively.  
The RMSE will be calculated with the following equation:

\begin{center}
$RMSE = \sqrt{\frac{1}{N}\sum(\hat{y}_{u,i}-y_{u,i})^2}$
\end{center}

## 3.1. Average Rating Model
```{r Model_Setup, include=FALSE, message=FALSE, warning=FALSE}
#Dividing edx to test and train sets
set.seed(1, sample.kind="Rounding") 
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]

#We remove the users and movies in the test set that do not appear in the training set
test_set <- test_set %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

#The average
mu <- mean(train_set$rating)

#RMSE function
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))}
```

In this model we will predict the same rating for all movies regardless of user. We want to minimize the RMSE, so in this case we will predict the average of all ratings for all movies and users $\mu$ (`r round(mu, digits = 2)`). Our model looks like this:

\begin{center}
$Y_{u,i} = \mu + \epsilon_{u,i}$
\end{center}

Where $\mu$ is the average of all ratings, the $\epsilon_{u,i}$ is the “noise” centered at 0 which describes the random variability.

```{r Avg_Rating_Model, include=FALSE, message=FALSE, warning=FALSE}
avg_rmse <- RMSE(test_set$rating, mu)

rmse_summary <- tibble(method = "Average Rating Model", RMSE = round(avg_rmse, digits = 4))
```


We collect the results of our models into one summary table:
```{r Table6_RMSE_Summary, echo=FALSE, message=FALSE, warning=FALSE}
rmse_summary %>% 
  knitr::kable(format = "pipe", caption = "Model accuracy", col.names = c("Method", "RMSE"))
```

## 3.2. Movie Effect Model
The next step to obtain more accuracy is to augment our first model with the movie effect term. This method is based on the observation, that some movies have higher ratings in general than the other. To address this movie effect, we introduce a term $b_{i}$ to represent the deviation of the average rating of movie i from the overall mean of ratings $\mu$.
Our second model will look like this:

\begin{center}
$Y_{u,i} = \mu + b_{i} + \epsilon_{u,i}$
\end{center}

Where $\mu$ is the average of all ratings, $b_{i}$ is the movie effect and the $\epsilon_{u,i}$ is the “noise”.

We plot the distribution of the $b_{i}$ values just to verify there are variability across movies, which means adding the movie effect to our model will improve our prediction.

```{r Plot4_bi, echo=FALSE, message=FALSE, warning=FALSE}
b_i <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))

b_i %>% ggplot(aes(b_i)) +
  geom_density(fill = "rosybrown1", alpha = 0.5) +
  labs(title="Movie Effect", x = "b_i", y = "Density")
```
We can see there is significant variation across movies.

```{r Movie_Effect_Model, include=FALSE, message=FALSE, warning=FALSE}
predicted_ratings <- test_set %>%
  left_join(b_i, by='movieId') %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)
  

b_i_rmse <- RMSE(test_set$rating,  predicted_ratings)

rmse_summary <-  rmse_summary %>% add_row(method = "Movie Effect Model", RMSE = round(b_i_rmse, digits = 4))
```
```{r Table7_RMSE_Summary2, echo=FALSE, message=FALSE, warning=FALSE}
rmse_summary %>% 
  knitr::kable(format = "pipe", caption = "Model accuracy", col.names = c("Method", "RMSE"))
```
We still did not obtain our goal, but we have a better RMSE compared to the first model. This means we are on the right track, but we need a bit more detailed model.

## 3.3. Movie and User Effect Model
To improve the accuracy, we can also take the user’s properties into account. Some users give higher ratings on average than the others. We augment our model with the user specific effect $b_{u}$. $b_{u}$ represents the deviation of the average rating of user u from the overall mean of ratings $\mu$.

\begin{center}
$Y_{u,i} = \mu + b_{i} + b_{u} + \epsilon_{u,i}$
\end{center}

We plot the average rating for users to see if there is variability across users.

```{r Plot5_bu, echo=FALSE, message=FALSE, warning=FALSE}
train_set %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) +
  geom_density(fill = "darkslategray2", alpha = 0.5) +
  labs(title="User Effect", x = "b_u", y = "Density")
```
We can see there is significant variation across users.

```{r Movie_User_Effect_Model, include=FALSE, message=FALSE, warning=FALSE}
b_u <- train_set %>%
  left_join(b_i, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- test_set %>%
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  mutate(pred  = mu + b_i + b_u) %>%
  pull(pred)

b_u_rmse <- RMSE(test_set$rating,  predicted_ratings)

rmse_summary <-  rmse_summary %>% add_row(method = "Movie and User Effect Model", RMSE = round(b_u_rmse, digits = 4))
```

```{r Table8_RMSE_Summary3, echo=FALSE, message=FALSE, warning=FALSE}
rmse_summary %>% 
  knitr::kable(format = "pipe", caption = "Model accuracy", col.names = c("Method", "RMSE"))
```

We can see this is our most accurate model yet. It provides RMSE = `r round(b_u_rmse, digits = 4)`. But unfortunately, we still not obtained our target RMSE.

## 3.4. Movie, User and Time Effect Model
The next effect we consider is the influence of the rating time of the movie on the ratings. Maybe the users were more lighthearted in the past, and became stricter as the time went on. We augment our model with the time specific effect $b_{t}$. bt represents the deviation of the average rating in the same year from the overall mean of ratings.

\begin{center}
$Y_{u,i,t} = \mu + b_{i} + b_{u} + b_{t} + \epsilon_{u,i,t}$
\end{center}

Lets see the density plot of $b_{t}$:

```{r Plot6_bt, echo=FALSE, message=FALSE, warning=FALSE}
train_set %>%
mutate(year = year(as_datetime(timestamp))) %>%
group_by(year) %>%
summarize(b_t = mean(rating)) %>%
ggplot(aes(b_t)) +
geom_density(fill = "cornsilk1", alpha = 0.5) +
labs(title="Time Effect", x = "b_t", y = "Density")

```
We can see there is some variation across years.

```{r Movie_User_Time Effect_Model, include=FALSE, message=FALSE, warning=FALSE}
b_t <- train_set %>%
  mutate(year = year(as_datetime(timestamp))) %>%
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by = "userId") %>%
  group_by(year) %>%
  summarize(b_t = mean(rating - mu - b_i - b_u))

predicted_ratings <- test_set %>%
  mutate(year = year(as_datetime(timestamp))) %>%
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  left_join(b_t, by='year') %>%
  mutate(pred = mu + b_i + b_u + b_t) %>%
  pull(pred)

b_t_rmse <- RMSE(test_set$rating,  predicted_ratings)

rmse_summary <-  rmse_summary %>% add_row(method = "Movie, User and Time Eff. Model", RMSE = round(b_t_rmse, digits = 4))
```

```{r Table9_RMSE_Summary4, echo=FALSE, message=FALSE, warning=FALSE}
rmse_summary %>% 
  knitr::kable(format = "pipe", caption = "Model accuracy", col.names = c("Method", "RMSE"))
```

The introduction of the time effect to our model decreased the RMSE very very slightly. We do not see any change in 4 decimals. Despite this, we keep the effect in the model, maybe we will have better luck on the validation set. To achieve the goal RMSE of this project, we need to introduce a more advanced technique in our final model.

## 3.5. Regularized Model
As we saw in chapter 2.6 there are several movies that were rated by only a few users. Therewith there are several users who rated only a few movies. And there are years when only a few movies were released. Predictions based on only a few ratings increase the uncertainty and therefore the inaccuracy of our model.
Regularization allows us to penalize these effects, so in our final model we will use the following formula:

\begin{center}
$Y_{u,i,t} = \mu + b_{i, reg} + b_{u, reg} + b_{t, reg} + \epsilon_{u,i,t}$
\end{center}


We introduce a penalty ($\lambda$) for $b_{i}$ that influenced by movies with very few ratings, for $b_{i}$  which is influenced by users who only rated a small number of movies and for $b_{t}$ that influenced by release years when only a few movies were released. In the above equation we noted these regularized factors with $b_{i, reg}$, $b_{i, reg}$ and $b_{i, reg}$ respectively. $\lambda$ is a tuning parameter, we can choose it to minimize the RMSE. To reduce computation time, we will tune $\lambda$ in two steps.  
At first, we will try $\lambda$s from a scale from 0 to 10, with whole number increments, then we will set a new narrower interval around the $\lambda$ that minimizes the RMSE, and we will use increments 0.2.

```{r Lamda_search_step1, include=FALSE, message=FALSE, warning=FALSE}
lambdas <- seq(0:10)

lambda_tuning_function <-  function(l){
  
  b_i_reg <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i_reg = sum(rating - mu)/(n()+l))
  
  b_u_reg <- train_set %>%
    left_join(b_i_reg, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u_reg = sum(rating - b_i_reg - mu)/(n()+l))
  
  b_t_reg <- train_set %>%
    mutate(year = year(as_datetime(timestamp))) %>%
    left_join(b_i_reg, by='movieId') %>%
    left_join(b_u_reg, by = "userId") %>%
    group_by(year) %>%
    summarize(b_t_reg = mean(rating - b_i_reg - b_u_reg - mu))
  
  predicted_ratings <-
    test_set %>%
    mutate(year = year(as_datetime(timestamp))) %>%
    left_join(b_i_reg, by = "movieId") %>%
    left_join(b_u_reg, by = "userId") %>%
    left_join(b_t_reg, by = "year") %>%
    mutate(pred = mu + b_i_reg + b_u_reg +b_t_reg) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))}

rmse_results <- sapply(lambdas, lambda_tuning_function)


##lambda that minimizes the RMSE
lambda_opt <- lambdas[which.min(rmse_results)]
```

The $\lambda$ that minimizes the RMSE at the first step is: `r lambda_opt`

```{r Plot7_Lambda_search_step1, echo=FALSE, message=FALSE, warning=FALSE, out.width="80%"}
qplot(lambdas, rmse_results)
```

So we set a new scale from `r lambda_opt-1` to `r lambda_opt+1` with increments 0.2.

```{r Lamda_search_step2, include=FALSE}
lambdas <- seq(lambda_opt-1, lambda_opt+1, 0.2)

rmse_results <- sapply(lambdas, lambda_tuning_function)

lambda_opt <- lambdas[which.min(rmse_results)]

reg_rmse <- min(rmse_results)

rmse_summary <-  rmse_summary %>% add_row(method = "Regularized Model", RMSE = round(reg_rmse, digits = 4))

```

Our final $\lambda$ is: `r lambda_opt` :)

```{r Plot8_Lambda_search_step2, echo=FALSE, message=FALSE, warning=FALSE, out.width="80%"}
qplot(lambdas, rmse_results)
```

```{r Table10_RMSE_Summary3, echo=FALSE, message=FALSE, warning=FALSE}
rmse_summary %>% 
  knitr::kable(format = "pipe", caption = "Model accuracy", col.names = c("Method", "RMSE"))
```

## 3.6. Final Touch
The predicted ratings cannot be less than 0.5 or greater than 5. Let’s see how many of the predicted ratings are outside of the valid range. 

```{r Outlier_search, include=FALSE, message=FALSE, warning=FALSE}
##The regularized model is needed to be run on different datasets during test and validation, so we create a function
regularized_model <- function(train_df, test_df, l){
  b_i_reg <- train_df %>%
    group_by(movieId) %>%
    summarize(b_i_reg = sum(rating - mu)/(n()+l))
  
  b_u_reg <- train_df %>%
    left_join(b_i_reg, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u_reg = sum(rating - b_i_reg - mu)/(n()+l))
  
  b_t_reg <- train_df %>%
    mutate(year = year(as_datetime(timestamp))) %>%
    left_join(b_i_reg, by='movieId') %>%
    left_join(b_u_reg, by = "userId") %>%
    group_by(year) %>%
    summarize(b_t_reg = mean(rating - b_i_reg - b_u_reg - mu))
  
  predicted_ratings <-
    test_df %>%
    mutate(year = year(as_datetime(timestamp))) %>%
    left_join(b_i_reg, by = "movieId") %>%
    left_join(b_u_reg, by = "userId") %>%
    left_join(b_t_reg, by = "year") %>%
    mutate(pred = mu + b_i_reg + b_u_reg + b_t_reg) %>%
    pull(pred)}

predicted_ratings <- regularized_model(train_set, test_set, lambda_opt)

##Search for outliers
outliers <- tibble(too_small = sum(predicted_ratings < 0.5), too_large = sum(predicted_ratings > 5))
```


```{r Table11_Outliers, echo=FALSE, message=FALSE, warning=FALSE}
outliers %>% 
  knitr::kable(format = "pipe", caption = "Prediction", col.names = c("< 0.5", "> 5"))
```

We can round the outside ratings to the nearest valid values.

```{r Capping, include=FALSE, message=FALSE, warning=FALSE}
predicted_ratings <- ifelse(predicted_ratings < 0.5, 0.5, predicted_ratings)
predicted_ratings <- ifelse(predicted_ratings > 5, 5, predicted_ratings)

capped_rmse <- RMSE(predicted_ratings, test_set$rating)

rmse_summary <-  rmse_summary %>% add_row(method = "Regularized Model (Capped)", RMSE = round(capped_rmse, digits = 4))

```

```{r Table12_RMSE_Summary5, echo=FALSE, message=FALSE, warning=FALSE}
rmse_summary %>% 
  knitr::kable(format = "pipe", caption = "Model accuracy", col.names = c("Method", "RMSE"))
```

## 3.6. Final result on validation set
Finally, we calculate our prediction on the "validation" set. Appropriate method to achieve better accuracy if we use the entire "edx" dataset as the training set, and use the optimal $\lambda$ parameter value we determined earlier. (Ref. link: [\textcolor{blue}{Harvardx Discussion}](https://courses.edx.org/courses/course-v1:HarvardX+PH125.9x+1T2021/discussion/forum/7762972aafb80108554fc7f48a4f4b5d75bd3f69/threads/605d094ce13629049eb18672))

```{r Validation_Setup, include=FALSE, message=FALSE, warning=FALSE}

##We remove the users and movies in the validation that do not appear in the training set
validation <- validation %>%
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
```

```{r Validation, include=FALSE, message=FALSE, warning=FALSE}
mu <- mean(edx$rating)

predicted_ratings <- regularized_model(edx, validation, lambda_opt)

predicted_ratings <- ifelse(predicted_ratings < 0.5, 0.5, predicted_ratings)
predicted_ratings <- ifelse(predicted_ratings > 5, 5, predicted_ratings)

final_rmse <- RMSE(predicted_ratings, validation$rating)

rmse_summary <-  rmse_summary %>% add_row(method = "Final RMSE", RMSE = round(final_rmse, digits = 4))
```


```{r Table13_RMSE_Summary6, echo=FALSE, message=FALSE, warning=FALSE}
rmse_summary %>% 
  knitr::kable(format = "pipe", caption = "Model accuracy", col.names = c("Method", "RMSE"))
```

# 4. Summary
We have successfully described a way to build up the recommendation algorithm to predict movie ratings using MovieLens 10M Dataset in this report. We started from the simplest possible algorithm, and progressively improved it by taking more and more effects into account. In our final model we took the overall average rating, the movie, user and the time effects into account, and used the regularization technique and some final improvement to obtain an acceptable RMSE.

The final goal was to achieve an RMSE less than 0.86490. The final RMSE from our algorithm is `r round(final_rmse, digits=4)`, so we achieved our project goal and the initial criteria of the HarvardX Data Science: Capstone project. 


# 5. Future Cosiderations
However we achieved the final goal of this project, our average error is still higher than 0.85 stars, so we have left room for further improvements. If in the future we will need even better accuracy, we can improve our final model for example with the following techniques. 

## 5.1 Genre Effects
Similar to the movie and user effects we could consider genre effect. We could analyze the average rating of the genres. Some genres may be rated higher on average than the others.
We could also consider  user+genre effects because of the fact that certain users prefer certain genres. Addressing these effect would improve the model performance.

## 5.2 Release Time Effect
The release time of the movie may have an effect on the ratings of the users. Maybe we are more lighthearted with the old classics, and stricter with newer movies. If we find correlation between the release time and the mean rating of a user, we could include it in our model.  
We could also consider the effect of the time difference between rating and the release of the movie on the ratings of users. As the saying goes: Time makes the memories nicer. Maybe we give higher ratings for movies that we saw a while ago.

## 5.3 Matrix Factorization
Matrix factorization is a highly effective, and widely used technique in building of recommendation systems. We could consider to build a model based on this technique. We could convert our dataset into a matrix. Each row of the matrix could be assigned to a user, and each column assigned to a movie. By using matrix factorization we could decompose our matrix to two smaller rectangular matrices, which could be handled much easier than the original large matrix. 

## 5.4 Rounding
We saw in Chapter 2.4, that the ratings before 2003 are scales from 1 to 5 star with whole star increments. The half star increments were introduced in 2003.
We could further improve the accuracy of the model by rounding our predictions to the nearest half-integer when predicting ratings from 2003 and later, and to nearest integer when predicting ratings before 2003.


